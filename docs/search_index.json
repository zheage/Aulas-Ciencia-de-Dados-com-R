[["introdução.html", "CIÊNCIA DE DADOS COM LINGUAGEM R Capítulo 1 Introdução", " CIÊNCIA DE DADOS COM LINGUAGEM R Richard Guilherme dos Santos Capítulo 1 Introdução Bem vindo! Sou formado em Matemática pela UNESP e atualmente faço mestrado em estatística pelo programa PIPGEs. Já me perguntei diversas vezes em como toda a teoria de probabilidade, estatística, inferência e de aprendizado de máquina é utilizada na prática, para resolver problemas que me deixavam curioso ou até mesmo competições de programação. Este livro vem para resolver este tipo de problema que chamou tanto minha atenção no passado. Este livro tem como objetivo servir como guia para as aulas do curso Ciência de Dados com R. Nele apresentaremos os conceitos de: Estatística Básica: Nesta parte do curso abordaremos conceitos de estatística como variáveis, tipos de distribuições discretas e contínuas, medidas descritivas e distribuição normal. Manipulação de dados no R: Neste tópico serão abordados as principais formas de manipulação de dados utilizando a linguagem R, com ênfase nas bibliotecas dplyr e tidyr. Além disso, abordaremos a criação de gráficos pelo pacote ggplot2. Modelos de Regressão Linear: Parte final do curso, onde o aluno aprenderá sobre diagrama de dispersão, coeficiente de correlação linear, regressão linear simples, múltipla e regressão logística, ganhando a capacidade de começar a criar modelos utilizando a linguagem R. "],["dados-utilizados.html", "Capítulo 2 Dados Utilizados 2.1 Motor Trend Car Road Test (mtcars) 2.2 Pokémons Dataset 2.3 Fifa 2022 Player Dataset", " Capítulo 2 Dados Utilizados Neste livro trabalharemos com diversos conjuntos de dados, nas próximas seções comentaremos brevemente sobre estes e a sua utilização. 2.1 Motor Trend Car Road Test (mtcars) Conjunto de dados nativo do R extraído da revista Motor Trend US de 1974. Possui diversos atributos de veículos dessa época e não precisa ser carregado de forma externa, estes já são salvos na variável mtcars. 2.2 Pokémons Dataset Presente em animações, jogos, filmes, séries e em toda a cultura nerd, Pokémon é uma série contendo os nossos monstrinhos favoritos! Utilizaremos aqui o conjunto com uma lista com todos os pokémons até então presente nos jogos, com seus nomes, tipos, status, classificação em lendário dentre outros atributos. Podemos encontrar o conjunto no site Kaggle. pokemon_stats &lt;- read_csv(&quot;G:/Meu Drive/Dados/pokemon_stats.csv&quot;) 2.3 Fifa 2022 Player Dataset FIFA é uma das principais franquias, se não a principal, de jogos de futebol, onde se concentra a maioria dos times e jogadores do esporte. Nos jogos cada jogador possui diferentes atributos e características que influenciam umas nas outras. Aqui consideraremos o conjunto de dados destes jogadores no jogo FIFA 22. As informações estão divididas em dois conjuntos e serão destaque quando trabalharmos sobre união e intersecção de conjuntos de dados. fifa_basic &lt;- read_csv(&quot;G:/Meu Drive/Dados/fifa_basic_info.csv&quot;) fifa_detailed &lt;- read_csv(&quot;G:/Meu Drive/Dados/fifa_detailed_info.csv&quot;) "],["introdução-ao-r.html", "Capítulo 3 Introdução ao R 3.1 Introdução a Fatores 3.2 Trabalhando com Datas", " Capítulo 3 Introdução ao R Aqui introduziremos alguns comandos da linguagem R, onde utilizamos funções para realizar operações que vão desde leitura e manipulação de dados a operações matemáticas. Comecemos criando um vetor de números: x &lt;- c(1,3,2,5) # x = c(1,3,2,5) # Também podemos utilizar &quot;=&quot; para atribuir variáveis x ## [1] 1 3 2 5 O comando acima combina os números 1,3,2 e 5 em um vetor de números e os salva em um objeto denominado x. Escrevemos x para recebermos os atributos do vetor. A partir disto podemos utilizar outras funções para calcularmos informações destes atributos, como o tamanho de um vetor: length(x) ## [1] 4 sua média: mean(x) ## [1] 2.75 também podemos realizar operações entre os vetores: a &lt;- c(1,2,3) b &lt;- c(2,3,4) a+b ## [1] 3 5 7 Há outros tipos de objetos que podem ser criados quando trabalhamos com R. Dentre os mais importantes para manipulação de dados estão as matrizes: mat = matrix(data = c(1,2,3,4), nrow = 2, ncol = 2, byrow = TRUE) mat ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 muitos devem já estar familiarizados com estas. A linguagem R fornece as mais diversas operações entre matrizes: a = matrix(data = 1:9, nrow = 3, ncol = 3) b = matrix(data = 1:9, nrow = 3, ncol = 3, byrow = T) # a + b # Soma de matrizes # a * b # Multiplicação dos elementos das matrizes termo a termo # a %*% b # Multiplicação de matrizes # t(a) # Transposta da matriz # det(a) # Determinante de uma matriz # solve(a) # Inversa da matriz sqrt(a) # Raiz quadrada dos elementos da matriz ## [,1] [,2] [,3] ## [1,] 1.000000 2.000000 2.645751 ## [2,] 1.414214 2.236068 2.828427 ## [3,] 1.732051 2.449490 3.000000 Funções aceitam os mais diversos tipos de argumentos. Para termos uma ideia de quais utilizarmos e seus respectivos atributos devemos fazer consultas em suas bibliotecas: help(matrix) Além disso, para armazenamento de dados temos os data.frames, tabelas que aceitam dados de tipos distintos: nomes = c(&#39;Carol&#39;, &#39;Alfredo&#39;, &#39;Godoberto&#39;) idade = c(18, 23, 19) peso = c(69, 75, 80) altura = c(1.70, 1.80, 1.85) ICM = peso/altura^2 df = data.frame(nomes, idade, peso, altura, ICM) df ## nomes idade peso altura ICM ## 1 Carol 18 69 1.70 23.87543 ## 2 Alfredo 23 75 1.80 23.14815 ## 3 Godoberto 19 80 1.85 23.37473 3.1 Introdução a Fatores Fatores são objetos para armazenamento de dados categóricos no R. A grande maioria dos algoritmos de aprendizado de máquina não trabalha com textos, mas sim com valores numéricos - transformar dados que estão armazenados como texto em fatores é um fator primordial para que os algoritmos os interpretem como dados categóricos. No conjunto de dados sobre pokémons, o Tipo de cada pokémon é armazenado por uma string (texto): str(pokemon_stats[&#39;Type_1&#39;]) ## tibble [721 × 1] (S3: tbl_df/tbl/data.frame) ## $ Type_1: chr [1:721] &quot;Grass&quot; &quot;Grass&quot; &quot;Grass&quot; &quot;Fire&quot; ... Se quisermos trabalhar de forma a utilizarmos essa coluna em algoritmos, ou até mesmo para certos pacotes que trabalham com dados categóricos, como ggplot2 para criação de certos gráficos, o ideal é realizar a transformação para fator: pokemon_stats$Type_1 = as.factor(pokemon_stats$Type_1) class(pokemon_stats$Type_1) # Classe dessa coluna passa a ser do tipo fator ## [1] &quot;factor&quot; Podemos transformar várias colunas em fatores utilizando a função apply: colunas = c(&#39;Type_1&#39;, &#39;Type_2&#39;) pokemon_stats[colunas] = lapply(pokemon_stats[colunas], factor) A diferença entre as funções da família apply é sútil. Estas variam com o tipo de dado esperado como entrada das funções e seu resultado, recomendo o post de Fernando Gama no medium sobre o assunto. 3.2 Trabalhando com Datas "],["medidas-descritivas.html", "Capítulo 4 Medidas Descritivas 4.1 Tipos de Variáveis 4.2 Medidas de Posição 4.3 Medidas de Dispersão 4.4 Quantis Empíricos 4.5 Box Plot 4.6 Transformações 4.7 Lab 01 - Conjunto de dados Iris 4.8 Lab 02 - Xadrez Brasil", " Capítulo 4 Medidas Descritivas Importante: A partir deste capítulo utilizaremos a função kable do pacote knitr para visualização de conjuntos de dados. Na prática isto não é necessário, apenas o realizamos para efeitos de visualização. 4.1 Tipos de Variáveis Antes de analisarmos conjuntos de dados, é necessário termos um conhecimento sobre tipos de variáveis. Para isto, consideremos a seguinte tabela: nome = c(&#39;Djoko&#39;,&#39;Wilson&#39;,&#39;Leon&#39;, &#39;Nilce&#39;) est_civil = c(&#39;Solteiro&#39;,&#39;Casado&#39;, &#39;Casado&#39;, &#39;Casado&#39;) escolaridade = c(&#39;Pós-graduação&#39;, &#39;Ensino médio completo&#39;, &#39;Pós-graduação&#39;, &#39;Superior completo&#39;) n_filhos = c(0, 0, 0, 0) salario = c(4500, 3000, 2000, 5500) idade = c(29, 33, 39, 32) df_youtubers = data.frame(nome, est_civil, escolaridade, n_filhos, salario, idade) kable(df_youtubers, align = &#39;c&#39;, caption = &#39;Dados sobre Youtubers.&#39;) # Melhor visualização dos dados para este PDF Table 4.1: Dados sobre Youtubers. nome est_civil escolaridade n_filhos salario idade Djoko Solteiro Pós-graduação 0 4500 29 Wilson Casado Ensino médio completo 0 3000 33 Leon Casado Pós-graduação 0 2000 39 Nilce Casado Superior completo 0 5500 32 Variáveis como sexo, escolaridade e estado civil apresentam realizações de uma qualidade ou atributo do indivíduo pesquisado, enquanto outras como número de filhos, salário e idade apresentam números como resultados de uma contagem ou mensuração. Chamamos as do primeiro tipo de qualitativas e as do segundo de quantitativas Cada uma das duas ainda pode ser dividida em dois tipos: Variável qualitativa nominal: atributos não apresentam uma ordem lógica; Variável qualitativa ordinal: atributos apresentam uma ordem lógica bem estabelecida; Variável quantitativa discreta: dados de contagem, assumem apenas valores inteiros; Variável quantitativa contínua: dados que podem assumir qualquer tipo de valor. Muitas vezes queremos resumir estes dados, apresentando um ou mais valores que sejam representativos da série toda. Neste contexto entram às medidas de posição e dispersão. 4.2 Medidas de Posição Usualmente utilizamos uma das seguintes medidas de posição (ou localização): média, mediana ou moda. Vamos as suas definições: A uma variável atribuiremos a letra \\(X\\) enquanto para seus elementos os valores \\(x_1, \\dots, x_n\\), sendo \\(n\\) o seu total de elementos. Moda: valor mais frequente do conjunto de valores observados. Mediana: valor que ocupa a posição central das observações quando estas estão ordenadas em ordem crescente. Quando o número de observações for par, usa-se como mediana a média aritmética das duas observações centrais. Na tabela 4.1 temos a seguinte mediana para a coluna salário: median(df_youtubers$salario) ## [1] 3750 Matematicamente ordenamos os dados do menor para o maior: \\(2000, 3000, 4500, 5500\\), selecionamos as observações centrais \\(3000\\) e \\(4500\\). Por fim, calculamos a média artimética de ambas, \\(\\frac{3000+4500}{2}\\), para obtermos a mediana. Além disso, podemos calcular a mediana para todas as colunas: # apply: aplica uma função a um conjunto de dados # MARGIN = 2: 1 para aplicar a função a todas as linhas e 2 a todas as colunas # FUN: função a ser aplicada ao conjunto de dados apply(df_youtubers[, c(&#39;n_filhos&#39;,&#39;salario&#39;,&#39;idade&#39;)], MARGIN = 2, FUN = median) ## n_filhos salario idade ## 0.0 3750.0 32.5 Média: soma de todos os elementos do conjunto dividida pela quantidade de elementos do conjunto \\[ \\overline{x} = \\frac{x_1+x_2 + \\dots + x_n}{n} \\] Na tabela 4.1 temos a seguinte média para o salário: mean(df_youtubers$salario) ## [1] 3750 Podemos calcular para todas as colunas que possuam valores numéricos: colMeans(df_youtubers[, c(&#39;idade&#39;, &#39;salario&#39;)]) ## idade salario ## 33.25 3750.00 4.3 Medidas de Dispersão O resumo de um conjunto de dados por uma única medida representativa de posição esconde toda a informação sobre a variabilidade de um conjunto de observações. Consideremos que cinco alunos realizaram cinco provas, obtendo as seguintes notas: nomes = c(&#39;alunoA&#39;, &#39;alunoB&#39;, &#39;alunoC&#39;, &#39;alunoD&#39;, &#39;alunoE&#39;) notas = matrix(c(3,4,5,6,7, 1,3,5,7,9, 2,5,5,5,8, 3,5,5,5,7, 0,0,5,10,10), nrow = 5, ncol = 5, byrow = T) df_alunos = data.frame(notas, row.names = nomes) colnames(df_alunos) = c(&#39;P1&#39;, &#39;P2&#39;, &#39;P3&#39;, &#39;P4&#39;, &#39;P5&#39;) kable(df_alunos, align = &#39;c&#39;) P1 P2 P3 P4 P5 alunoA 3 4 5 6 7 alunoB 1 3 5 7 9 alunoC 2 5 5 5 8 alunoD 3 5 5 5 7 alunoE 0 0 5 10 10 Temos as seguintes médias para os alunos: # Podemos ver a média de cada coluna utilizando colMeans(df_alunos) rowMeans(df_alunos) ## alunoA alunoB alunoC alunoD alunoE ## 5 5 5 5 5 Cada aluno possui a mesma média de notas, porém, isto não informa nada sobre a diferença na variabilidade das notas. A partir disto, são criadas medidas que sumarizam a variabilidade de um conjunto de observações. Uma primeira ideia é considerar a soma das diferença dos dados em relação a média: \\[ x_1 - \\overline{x} + x_2 - \\overline{x} + \\cdots + x_n - \\overline{x} \\] Porém, podemos mostrar que em qualquer conjunto a soma destes desvios é igual a zero. Uma alternativa é então adicionar o valor absoluto em cada diferença: \\[ |x_1 - \\overline{x}| + |x_2 - \\overline{x}| + \\cdots + |x_n - \\overline{x}| \\] Apesar de possuir uma boa interpretabilidade, tal métrica não possui propriedades matemáticas interessantes. Assim, estatísticos trabalham com a diferença dos dados em relação a média ao quadrado: \\[ (x_1 - \\overline{x})^2 + (x_2 - \\overline{x})^2 + \\cdots + (x_n - \\overline{x})^2 \\] Como muitas vezes queremos comparar conjuntos de dados de diferentes tamanhos, realizamos a divisão desta soma pelo total de elementos em uma amostra e a este número chamamos de variância: \\[ \\text{var}(X) = \\frac{(x_1 - \\overline{x})^2 + (x_2 - \\overline{x})^2 + \\cdots + (x_n - \\overline{x})^2}{n} \\] E a partir disto, definimos desvio padrão como sendo a raiz da variância: \\[ \\text{dp} = \\sqrt{\\text{var}(X)} \\] Realizamos isto pois caso os dados estejam em uma certa unidade de medida, como \\(cm\\) , ao calcularmos a variância passamos a trabalhar com \\(cm^2\\), o que dificulta a interpretabilidade dos resultados. Utilizando o valor na raiz quadrada, voltamos a trabalhar com a unidade de medida utilizada. 4.4 Quantis Empíricos Tanto a média como o desvio padrão podem não ser medidas adequadas para representar um conjunto de dados, uma vez que: São afetados por valores extremos; Apenas os dois valores não dão informação sobre a simetria ou assimetria da distribuição dos dados Vimos que a mediana define uma divisão dos dados em duas metades. Além dela existem medidas chamadas de quantil de ordem p ou p-quantil indicado por \\(q(p)\\) onde \\(p\\) é uma proporção qualquer, \\(0&lt;p&lt;1\\) tal que \\(100\\%\\) das observações sejam menores do que \\(q(p)\\). Abaixo temos alguns dos nomes dos quantis mais utilizados: \\(q(0.25) = q_1:\\) 1° Quartil ou 25° Percentil \\(q(0.50) = q_2:\\) 2° Quartil, Mediana ou 50° Percentil \\(q(0.75) = q_3:\\) 3° Quartil ou 75° Percentil \\(q(0.40) 1:\\) 4° Decil \\(q(0.95):\\) 95° Percentil No R podemos visualizar os quartis da seguinte forma: quantile(df_alunos$P1) ## 0% 25% 50% 75% 100% ## 0 1 2 3 3 Em várias colunas: apply(df_alunos, 2, quantile, seq(0,1,.2)) ## P1 P2 P3 P4 P5 ## 0% 0.0 0.0 5 5.0 7.0 ## 20% 0.8 2.4 5 5.0 7.0 ## 40% 1.6 3.6 5 5.6 7.6 ## 60% 2.4 4.4 5 6.4 8.4 ## 80% 3.0 5.0 5 7.6 9.2 ## 100% 3.0 5.0 5 10.0 10.0 4.5 Box Plot A informação contida nos quantis pode ser confusa quando estamos observando vários conjuntos de dados. A partir disto a traduzimos em um diagrama, qual é chamado de box plot: Para construção dessa gráfico definimos por intervalo interquartil o valor: \\[ \\text{IQ}(X) = q_3 - q_1 \\] Desenhamos um retângulo que parte do primeiro quartil até o terceiro, com a mediana sendo representada por uma linha em seu interior. A partir do retângulo desenhamos uma linha até o maior ponto que não exceta o valor \\(q_3+1.5 \\cdot \\text{IQ}(X)\\), chamado de limite superior. De modo análogo fazemos o mesmo procedimento até a parte inferior do retângulo considerando o valor \\(q_1 + 1.5 \\cdot \\text{IQ}(X)\\) chamado de limite interior. As observações que estiverem acima do limite superior ou abaixo do limite superior são chamados de pontos exteriores e representadas por asteriscos. Essas observações podem ser chamaas de outliers ou valores atípicos. Modo simples de como realizar um boxplot pelo R: boxplot(df_alunos, xlab = &quot;Provas&quot;, ylab = &quot;Notas&quot;, main = &quot;Boxplot dos alunos&quot;) O aluno mais atento pode se perguntar: porque alguns dos boxplots não possuem a linha superior e/ou inferior? Isto ocorre quando temos muitos dados em uma mesma categoria, com o primeiro ou terceiro quartil tendo o mesmo valor que o mínimo ou máximo do conjunto de dados: apply(df_alunos, 2, quantile) ## P1 P2 P3 P4 P5 ## 0% 0 0 5 5 7 ## 25% 1 3 5 5 7 ## 50% 2 4 5 6 8 ## 75% 3 5 5 7 9 ## 100% 3 5 5 10 10 O box plot dá uma ideia de posição, dispersão, assimetria dos dados. hist(df_alunos$P4, breaks = seq(5, 10, 0.5)) 4.6 Transformações Vários procedimentos estatísticos são baseados na posição que os dados possuem uma distribuição em forma de sino (distribuição normal) ou que a distribuição seja mais ou menos simétrica: # Simula 500 dados de uma distribuição normal dados_normal &lt;- rnorm(n = 10000) # Gráfico de suas frequências hist(dados_normal) Se quisermos utilizar tais procedimentos podemos efetuar transformações nas observações, de modo a se obter uma distribuição mais simétrica e próxima da normal. As transformações mais frequentemente utilizadas são: \\[ x = \\left\\{\\begin{matrix}&amp;\\sqrt{x}\\\\ &amp;\\ln(x) \\\\&amp;\\frac{1}{x}\\end{matrix}\\right. \\] para cada transformação obtemos gráficos apropriados para os dados originais e transformados, de modo a escolhermos o valor mais adequado de \\(p\\). dados_gamma &lt;- rgamma(n = 300, shape = 1) par(mfrow = c(2,2)) # MultiFrame rowwise layout hist(dados_gamma) hist(sqrt(dados_gamma)) hist(1/dados_gamma) hist(log(dados_gamma)) 4.7 Lab 01 - Conjunto de dados Iris O conjunto de dados Iris é um dos mais utilizados quando introduzimos conceitos de ciência de dados. Este pode ser encontrado em UCI Machine Learning Repository. Tal conjunto consiste de 150 amostras de 4 tipos de espécies de flores distintas contendo os atributos: SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm Podemos acessá-lo no R sem nenhum carregamento prévio da seguinte forma: # A função head() mostra os cinco primeiros itens de data.frame: head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa Há certas boas práticas ao carregar um conjunto de dados, dentre elas temos: Visualização de sua dimensão: # O primeiro valor é a quantidade de linhas do conjunto de dados # e o segundo a sua quantidade de atributos dim(iris) ## [1] 150 5 Visualização do tipo de cada atributo: str(iris) # Structure of an Arbitrary R Object ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Sumário de seus atributos: summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## Dessa maneira poderemos contatar valores errôneos no conjunto de dados, distribuições de variáveis categóricas e ter um melhor contato com o conjunto de dados. Há ainda diversas maneiras de realizarmos visualizações desse conjunto no R, observemos o boxplot da variável Sepa.Length: boxplot(iris$Sepal.Length) Observamos que não há presença de outliers, além disso, como a parte debaixo do retângulo separado pela linha que representa a mediana é menor, isto indica que a distribuição dos dados é ligeiramente assimétrica, o qual é confirmado pelo histograma: hist(iris$Sepal.Length) ggplot(data = iris, aes(x = Sepal.Length, fill = ..count..)) + geom_histogram(binwidth = 0.25, boundary = 0) + scale_x_continuous(breaks = seq(1, 10, by = 0.25)) 4.8 Lab 02 - Xadrez Brasil Em 2022 houve uma polêmica no universo de xadrez entre o atual campeão mundial de xadrez Magnus Carlsen e o jovem grande mestre Hanns Niemann, qual possui um histórico de trapaças e foi acusado de repetir estes atos no torneio. Magnus chegou a abandonar o torneio motivado por acreditar que seu oponente utilizava de engines em sua partida. Baseado nisso vários estatísticos se debrulharam entre as partidas de Niemann com o objetivo de encontrar evidências de sua rápida ascensão no xadrez, qual bateu recordes como grande mestre mais rápido da história. Uma destas analises foi realizada pelo canal brasileiro de youtube Xadrez Brasil, e nela podemos observar como medidas de posição e dispersão podem ser utilizadas para fortalecer ou não a hipótese de que Niemann estava trapaçeando. https://www.youtube.com/watch?v=60QPEGsOCyw "],["introdução-ao-tidyverse.html", "Capítulo 5 Introdução ao Tidyverse", " Capítulo 5 Introdução ao Tidyverse Pacotes são conjuntos de códigos criados para a linguagem R de forma que as mais diversas funcionalidades sejam simplificadas e padronizadas. Para trabalharmos com análise de dados utilizaremos pacotes do tidyverse, uma coleção de pacotes utilizados para manipulação de dados que compartilham uma filosofia em comum e são projetados para serem trabalhados em conjunto. Muitos deles não são nativos da linguagem e devem ser instalados utilizando a função install.packages. Neste capítulo trabalharemos com: Dplyr: Manipulação de conjunto de dados Tidyr: Modelagem no formato de conjuntos de dados GGPlot2: Visualização de conjuntos de dados Stringr: Manipulação de texto Forcats: Manipulação de fatores Inicialmente devemos realizarmos a intalação dos pacotes: install.packages(&#39;dplyr&#39;) install.packages(&#39;tidyr&#39;) install.packages(&#39;ggplot2&#39;) install.packages(&#39;stringr&#39;) install.packages(&#39;forcats&#39;) # install.packages(&#39;tidyverse&#39;) Instala todos os pacotes do tidyverse de uma vez e em seguida podemos carregar os pacotes no R: library(dplyr) # Manipulação de dados library(tidyr) # Modificação no formato de conjuntos de dados library(ggplot2) # Visualização de gráficos library(stringr) library(forcats) library(readr) # Biblioteca para leitura de dados Também podemos carregar vários em uma linha de código utilizando a biblioteca easypackages: install.packages(&#39;easypackages&#39;) # Instalar a biblioteca library(easypackages) # Carrega o pacote libraries(&#39;dplyr&#39;, &#39;tidyr&#39;, &#39;ggplot2&#39;, &#39;stringr&#39;, &#39;forcats&#39;, &#39;readr&#39;) Trabalharemos especialmente com Data Munging, processo de preparar conjuntos de dados para relatórios e análises. Esta parte incorpora todas as etapas anteriores à análise, incluindo estruturação de dados, limpeza, enriquecimento e validação. "],["readr.html", "Capítulo 6 Readr", " Capítulo 6 Readr "],["dplyr-tidyr.html", "Capítulo 7 Dplyr &amp; Tidyr 7.1 Extração de Observações (Linhas) 7.2 Extração de Variáveis (Colunas) 7.3 Resumir dados 7.4 Criação de Novas Variáveis 7.5 Combinar Conjuntos de Dados 7.6 Agrupar Dados (group_by) 7.7 Remodelando Dados", " Capítulo 7 Dplyr &amp; Tidyr Dplyr e Tidyr são pacotes que se complementam na manipulação de dados. O primeiro foca na realização de manipulação enquanto o segundo na modificação de seu formato. Aqui utilizaremos como referência a folha de referência de ambas as bibliotecas. Além disso recomendo a consulta do site Rdocumentation para visualização de todas as funções dos pacotes. Vamos então aos principais recursos das bibliotecas. 7.1 Extração de Observações (Linhas) 7.1.1 filter Muitas vezes queremos selecionar algumas linhas que satisfaçam um critério lógico. Consideremos o conjunto mtcars, o número de cilindros de cada carro varia entre 4, 6 e 8 Cyl Freq 4 11 6 7 8 14 Caso estejamos interessados em selecionar apenas os carros com 4 cilindros primeiro utilizamos o operador lógico == para verificarmos a igualdade de um vetor em relação a um valor. Caso na n-ésima linha a condição seja satisfeita, a n-ésima posição do vetor será retornada como TRUE. Caso contrário teremos um FALSE nesta posição. condic_logica &lt;- mtcars$cyl == 4 condic_logica ## [1] FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE ## [13] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE FALSE FALSE FALSE ## [25] FALSE TRUE TRUE TRUE FALSE FALSE FALSE TRUE Após isto podemos então utilizar a função filter. O primeiro parâmetro são os dados a serem filtrados enquanto o segundo é a condição lógica utilizada. filter(.data = mtcars, condic_logica) Podemos aplicar mais de uma condição lógica de duas formas. A primeira é passarmos múltiplos argumentos através da função filter. A segunda é utilizarmos os operadores lógicos “e” (&amp;) e “ou” (|). &amp;: Ambas as condições precisam ser satisfeitas; |: Apenas uma das condições precisa ser satisfeita. Caso queiramos selecionar os carros com 4 cilindros e mais do que 100 cavalos de potência: filter(.data = mtcars, mtcars$cyl == 4, mtcars$hp &gt; 100) # Também podemos realizar #filter(.data = mtcars, mtcars$cyl == 4 &amp; mtcars$hp &gt; 100) Caso queiramos selecionar os carros com 4 cilindros ou mais do que 100 cavalos de potência: filter(.data = mtcars, mtcars$cyl == 4 | mtcars$hp &gt; 100) Há uma diferença clara entre os operadores. No primeiro queremos que ambas as condições sejam satisfeitas enquanto no segundo, queremos qualquer linha que tenha uma delas satisfeita. 7.1.2 distinct Remove linhas duplicadas do conjunto de dados. alunos = data.frame(Alunos = c(&quot;Gabriel&quot;, &quot;Gabriel&quot;, &quot;Renato&quot;)) distinct(alunos) ## Alunos ## 1 Gabriel ## 2 Renato 7.1.3 Operador Pipe A ideia do operador pipe, %&gt;% é bem simples, usar o valor resultante da expressão do lado esquerdo como primeiro argumento da função do lado direito. mtcars %&gt;% filter((mtcars$cyl == 4) &amp; (mtcars$hp &gt; 100)) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.9 1 1 5 2 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.6 1 1 4 2 Também podemos passar a expressão do lado esquerdo como um certo parâmetro de u margumento da função, evidenciando por . no parâmetro que queremos. Isto ajuda a tornar o código mais legível quando analisado por outras pessoas (ou nós mesmos em outros dias). A partir daqui sempre utilizaremos o operador pipe! 7.1.4 sample_frac Seleciona uma fração das linhas do conjunto de dados aleatoriamente mtcars %&gt;% sample_frac(0.7, replace = FALSE) # Se replace = TRUE então uma mesma linha pode ser selecionada múltiplas vezes 7.1.5 slice Seleciona linhas pela posição # Vamos selecionar as linhas 10, 12, 14, 16, 18 e 20: mtcars %&gt;% slice(seq(10, 20, 2)) 7.2 Extração de Variáveis (Colunas) A seleção de colunas é feita sxclusivamente com a função select. Nela passamos todas as colunas que queremos selecionar em um conjunto de dados: mtcars %&gt;% select(mpg, cyl, hp) Leitores mais atentos podem estar se perguntando a vantagem de se utilizar select comparado aos métodos nativos da própria linguagem R. A mais óbvia é por sua legibilidade no conjunto de dados, mas esta não é a única. Podemos utilizar de funções auxiliares para seleção de colunas. Isto pode não ser necessário quando trabalhamos com conjunto de dados pequenos, mas imagine trabalhando bom dados que possuem mais do que 500 colunas, o que é bem comum quando trabalhamos com Big Data. Vamos a alguns exemplos: # Seleciona as cinco primeiras colunas mtcars %&gt;% select(1:5) # Seleciona colunas cujo nome contém alguma string: mtcars %&gt;% select(contains(&quot;a&quot;)) # Seleciona todas as colunas entre hp e gear: mtcars %&gt;% select(hp:gear) # Seleciona todas as colunas, menos hp mtcars %&gt;% select(-hp) 7.3 Resumir dados Resumo de dados consiste na utilização de métricas como média, mediana ou variância para termos melhor informações dos nossos dados. Dentre estas podemos utilizar: min: menor valor de um vetor; max: maior valor de um vetor; mean: média de um vetor; median: mediana de um vetor; var: variância de um vetor; sd: desvio padrão de um vetor. Podemos aplicar estas funções em múltiplas colunas ou aplicarmos várias em colunas distintas. Vamos aos exemplos. 7.3.1 summarise Resume os dados em uma única linha de valores pokemon_stats %&gt;% summarise(tot_mean = mean(Total), tot_std = sd(Total)) ## # A tibble: 1 × 2 ## tot_mean tot_std ## &lt;dbl&gt; &lt;dbl&gt; ## 1 418. 110. É melhor utilizado quando trabalhamos com a função group_by. 7.3.2 summarise_each Aplica a função em cada coluna dos dados: pokemon_stats %&gt;% select(Total:Speed) %&gt;% # Vamos selecionar apenas algumas colunas de atributos numéricos dos dados summarise_each(mean) ## Warning: `summarise_each_()` was deprecated in dplyr 0.7.0. ## Please use `across()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. ## # A tibble: 1 × 7 ## Total HP Attack Defense Sp_Atk Sp_Def Speed ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 418. 68.4 75.0 70.8 68.7 69.3 65.7 7.3.3 count Conta o número de linhas com cada valor único de uma variável count(mtcars, cyl) # Similar ao table() ## cyl n ## 1 4 11 ## 2 6 7 ## 3 8 14 7.4 Criação de Novas Variáveis Criação de novas colunas nos conjunstos de dados. 7.4.1 mutate Calcula e acrescenta uma ou mais colunas # Vamos criar uma coluna retornando TRUE se o pokémon tiver Speed &gt; 100 # e Attack ou Sp_Atk maior que 100: pokemon_stats %&gt;% mutate(Atacante = ifelse((Attack &gt; 100 &amp; Speed &gt; 100) | (Sp_Atk &gt; 100 &amp; Speed &gt; 100), TRUE, FALSE)) 7.4.2 transmute Calcula uma ou mais novas colunas, removendo as originais. Útil quando queremos transformar todas as colunas de um conjunto de dados, removendo as colunas originais. Vamos supor que só estamos interessados apenas no nome, tipo de corpo e IMC de cada pokémon: pokemon_stats %&gt;% transmute(Nome = Name, Body_Style = Body_Style, IMC = round(Weight_kg / Height_m^2,2)) 7.5 Combinar Conjuntos de Dados Muitas vezes queremos trabalhar com dados que estão em arquivos distintos. Isto é comum em bancos de dados, onde várias informações são guardadas em diferentes arquivos. Nos exemplos abaixo iremos trabalhar com os dados do conjunto FIFA, que apresentam a variável ID como indicação de jogadores. Quem já trabalhou com conjuntos no Ensino Médio ou até mesmo teve um primeiro contato com a linguagem SQL já ouviu falar sobre tipos de join. Estes são formas de unir tabelas a partir de uma certa ordem e se dividem em quatro tipos: left, right, inner e full: Tipos de Join. A diferença entre eles é qual conjunto de dados será utilizado como referência para a união de ambos. 7.5.1 left_join A primeira tabela é utilizada como referência na hora de combinar ambos os conjuntos. Assim todas as linhas do primeiro parâmetro serão combinadas com a utilizada como segundo: # No parâmetro &quot;by&quot; passamos por a variável que queremos realizar o agrupamento left_join(fifa_detailed, fifa_basic, by = &#39;ID&#39;) A lógica do right_join é análoga, sendo o conjunto de dados do segundo parâmetro como fixo: # No parâmetro &quot;by&quot; passamos por a variável que queremos realizar o agrupamento right_join(fifa_detailed, fifa_basic, by = &#39;ID&#39;) Ambos os resultados não são iguais: O primeiro código retorna um conjunto de dados com 14705 observações enquanto o segundo com 19825 observações! 7.5.2 inner_join Une ambos os conjuntos, porém apenas linhas presentes em ambos são consideradas: inner_join(fifa_detailed, fifa_basic, by = &#39;ID&#39;) 7.5.3 full_join Une os dados mantendo todos os valores e todas as linhas. Valores faltantes são atribuídos a NA fifa_merged &lt;- full_join(inner_join(fifa_detailed, fifa_basic, by = &#39;ID&#39;), fifa_basic, by = &#39;ID&#39;) 7.6 Agrupar Dados (group_by) Agrupa variáveis de acordo com um ou mais categoria. A função em si não modifica o conjunto, mas as próximas serão modificadas pela forma que os dados estão agrupados. Vamos calcular a média de status de pokémons pelos seus primeiros tipos: pokemon_stats %&gt;% group_by(Type_1) %&gt;% # Agrupa por Tipo summarise(Tot_mean = mean(Total)) 7.7 Remodelando Dados Mudar o formato e configuração dos dados permite separar variáveis que estão em uma coluna em mais de uma ou unir várias colunas em uma. Além disso podemos mudar o nome de colunas ou até mesmo ordenar seus valores de forma crescente. A biblioteca tidyr é perfeita para para isto, imagine-se trabalhando com a variável mês e dia em um conjunto de dados. Em vários algoritmos é interessante considerarmos o mês e o dia como atributos separados, assim cada um é alocado a uma coluna do conjunto de dados para que possamos realizar o ajuste dos algoritmos. 7.7.1 arrange Ordena linhas pelos valores de uma coluna (menor para o maior). Vamos utilizar como exemplo a ordenação da média dos atributos do pokémon por tipo: pokemon_stats %&gt;% group_by(Type_1) %&gt;% # Agrupa por Tipo summarise(Tot_mean = mean(Total)) %&gt;% arrange(Tot_mean) Caso estejamos interessados em ordenar os valores do maior para o menor utilizamos a função desc para a coluna dentro da função arange: pokemon_stats %&gt;% group_by(Type_1) %&gt;% # Agrupa por Tipo summarise(Tot_mean = mean(Total)) %&gt;% arrange(desc(Tot_mean)) 7.7.2 rename Renomeia colunas do conjunto de dados pokemon_stats %&gt;% rename(Nome = Name, Tipo_I = Type_1, Tipo_2 = Type_2) 7.7.3 separate Para as próximas funções utilizaremos os dados de fifa_deitaled, em particular a coluna Work Rate. A função separate separa uma coluna em uma ou mais linhas fifa_separate &lt;- fifa_detailed %&gt;% separate(col = `Work rate`, into = c(&quot;Work1&quot;, &quot;Work2&quot;), sep = &quot;/&quot;) 7.7.4 unite Une várias colunas em uma. Vamos considerar a coluna Work1 e Work2 que separamos anteriormente. Poderíamos retorná-las ao estado anterior da seguinte forma: fifa_separate %&gt;% unite(col = &quot;Work rate&quot;, c(Work1, Work2), sep = &quot;-&quot;) %&gt;% select(&quot;Work rate&quot;) "],["ggplot2.html", "Capítulo 8 GGPlot2 8.1 Diagrama de Dispersão (geom_point) 8.2 Centralizando o Título de Gráficos 8.3 Gráficos de Barras (geom_bar e geom_col) 8.4 Gráficos de Linhas (geom_line) 8.5 Gráfico de Funções (stat_function) 8.6 Histogramas (geom_histogram) 8.7 Boxplots (geom_boxplot) 8.8 Facets (facet_grid) 8.9 Bibliotecas Auxiliares", " Capítulo 8 GGPlot2 Pacote para criação de gráficos baseado no livro The Grammar of Graphics, que destrincha todas as componentes de um gráfico estatístico em partes individuais. Grammar of Graphics. Vamos aos componentes eu a biblioteca se baseia: Data: Dados que serão utilizados para visualização; Aesthetic (Estética): Argumentos para realizar a visualização dos dados; x, y: variável em cada eixo; colour: variável para colorir objetos geométricos ; fill: variável para colorir dentro de objetos geométricos; group: forma em que os dados podem ser agrupados; shape: formato dos pontos; linetype: tipo de linha a ser utilizado; size: escala utilizada para pontos, retângulos; alpha: transparência dos objetos geométricos. Objetos geométricos: determinam o tipo de gráfico; Facets: para múltiplos gráficos diferindo por grupos; Statistics: Informações estatísticas como médias, quantidade de ocorrências; Cordenadas: Tipo de coordenada, pode ser cartesiana, polar e de projeção; Temas: Fonte, cores, tamanho, formato dos objetos do gráfico. Observemos então como ggplot2 utiliza desta teoria na prática. Caso o leitor queira se aprofundar nos conceitos trabalhados aqui ou busque por algum gráfico em específico recomendamos R Graphics Cookbook, 2nd edition. 8.1 Diagrama de Dispersão (geom_point) Inicialmente devemos carregar os dados ao gráfico pela função ggplot, podendo ou não já atribuir uma estética: # A variável wt será considerada para o eixo x # A variável mpg será considerada para o eixo y ggplot(data = mtcars, aes(x = wt, y = mpg)) # ggplot(data = mtcars) também é uma opção válida! # Porém aqui os eixos não serão criados Note que apesar de termos um título e os eixos do gráfico, ainda não temos nenhuma informação! Isto acontece pois precisamos escolher o tipo de gráfico que queremos utilizar. Para isto, utilizamos o análogo da biblioteca dplyr, para a concatenação de funções na biblioteca ggplot2: o operador de soma + Criemos então um diagrama de dispersão, também conhecido como gráfico de pontos ou scatter plot: ggplot(data = mtcars, aes(x = wt, y = mpg)) + geom_point() Uma das vantagens da biblioteca é a fácil customização de seus gráficos. Abaixo observamos o gráfico anterior, mas agora trazendo a informação de quantidade de cilindros nas cores e tamanho dos pontos: # Pacote com algumas paletas de cores: library(RColorBrewer) # https://r-graph-gallery.com/38-rcolorbrewers-palettes.html # Algumas propriedades do aesthetic se dão melhor trabalhando com fatores: mtcars$cyl &lt;- as.factor(mtcars$cyl) # Vamos então a criação do gráfico em si ggplot(data = mtcars, aes(x = wt, y = mpg, size = cyl, color = cyl)) + geom_point() + # O R já vem com diversos temas já prontos, podemos visualizá-los em: # https://ggplot2.tidyverse.org/reference/ggtheme.html theme_bw() + # labs é uma função padrão para mudar o nome do título ou de eixos do gráfico: labs(title = &#39;Diagrama de Dispersão&#39;) + # theme modifica diversas propriedades estéticas nos gráficos, aqui estamos interessados em centralizar nosso título: theme(plot.title = element_text(hjust = 0.5)) + # A biblioteca RColorBrewer traz diversas opções para modificar a paletta # de cores dos gráficos. # Esta pode ser de forma automática: #scale_colour_brewer(palette = &#39;Dark2&#39;) # Ou de forma manual: scale_color_manual(values = c(&#39;#EB2E23&#39;, &#39;#29E2F2&#39;, &#39;#313FDB&#39;)) # A escolha de paletas de cores dá uma aula por si só! # É importante estar atento a algumas considerações: # 1. Trabalhar com a mesma paleta o projeto inteiro # 2. A paleta precisa fazer sentido para o projeto/tipo # de propriedade que está informando # 3. As cores não podem ser muito saturadas ou de difícil visualização # Abaixo temos dois sites que utilizo bastante para criação de paletas de cores: # https://color.adobe.com/pt/create/color-wheel # https://paletadecores.com 8.2 Centralizando o Título de Gráficos A visualização de gráficos pode causar certa estranheza quando este está desalinhado ou mal formatado. Uma das configurações iniciais que recomendo ao realizarmos a construção é a execução do código abaixo: theme_update(plot.title = element_text(hjust = 0.5)) A função theme_update altera as configurações de gráfico padrões nos próximos gráficos realizados. plot.title altera configurações em relação a posição do título, enquanto element_text(hjust = 0.5) informa que queremos os títulos centralizados no centro dos gráficos. Por padrão os gráficos de ggplot2 não tem títulos centralizados e, por questões estéticas, recomendo deixá-los alinhados ao centro. Outras configurações podem ser adicionadas como padrão quando estivermos trabalhando, mas começaremos com esta. 8.3 Gráficos de Barras (geom_bar e geom_col) Na biblioteca ggplot2 temos dois tipos de gráficos de barras. O primeiro, geom_bar, é utilizado quando queremos que o eixo \\(y\\) seja a frequência de alguma categoria ou grupo, enquanto o segundo, geom_col, quando o eixo \\(y\\) for uma coluna do conjunto de dados. Vamos utilizar a função geom_bar para visualizar a frequência de Tipos de pokémon no conjunto pokemon_stats: ggplot(data = pokemon_stats, aes(x = Type_1)) + geom_bar() Na prática queremos de um pouco mais de vida nos nossos gráficos, vamos formatá-lo para que este fique visualmente mais atrativo: # Vamos atribuir cores a cada atributo utilizando `fill` na estética: ggplot(data = pokemon_stats, aes(x = Type_1, fill = Type_1)) + geom_bar() + # Como cada barra é um atributo diferente, a legenda não faz muito sentido # e dessa forma será retirada: theme(legend.position = &#39;none&#39;) + # É difícil visualizar a quantidade exata de cada barra no gráfico anterior # por isso vamos atribuir a quantidade de cada categoria em cima de cada barra: geom_text(aes(label = ..count..), stat = &quot;count&quot;, color = &#39;black&#39;, vjust = -1) + # Alterar o eixo y para que o texto anterior não fique cortado: ylim(0,110) + # Por fim, vamos retirar os valores do eixo y: theme(axis.text.y=element_blank(), axis.ticks.y=element_blank()) + # e formatar o nome dos eixos e título: xlab(&#39;Tipo Primario&#39;) + ylab(&#39;Quantidade&#39;) + ggtitle(&#39;Quantidade de Pokemons por Tipo&#39;) Alunos mais ansiosos podem estar pensando: MEU DEUS! COMO EU VOU LEMBRAR DE TODAS ESSAS CONFIGURAÇÕES?! Na realidade nós não lembramos de todas elas de cara. Grande parte das funções acabamos por decorar pela experiência com a linguagem, enquanto outras apenas realizamos uma busca rápida para resolver nosso problema. Além disso, podemos utilizar dados obtidos por operações no dplyr como entrada para os gráficos do ggplot2. Aqui vamos criar um conjunto de dados com a média de cada Tipo de pokémon e então criar um gráfico de barras com a função geom_col com o valor para cada barra: # Pré-processamento dos dados. Aqui utilizaremos a biblioteca dplyr # do capítulo anterior para termos um conjunto com a média do total # de status de pokémons por tipo: pokemon_stats %&gt;% select(Type_1, Total) %&gt;% group_by(Type_1) %&gt;% summarise(tot_mean = mean(Total)) %&gt;% arrange(tot_mean) %&gt;% # Aqui a função pipe, %&gt;%, também pode ser utilizada para informar que o conjunto de dados da esquerda será o input para a função da direita! ggplot(aes(x = Type_1, y = tot_mean)) + geom_col() Novamente, podemos melhorar e muito este gráfico! # Pré-processamento dos dados pokemon_stats %&gt;% select(Type_1, Total) %&gt;% group_by(Type_1) %&gt;% summarise(tot_mean = mean(Total)) %&gt;% arrange(tot_mean) %&gt;% ggplot(aes(x = Type_1, y = tot_mean, fill = Type_1)) + geom_col() + # Retirada da legenda theme(legend.position = &#39;none&#39;) + # Adiciona os valores de cada barra geom_text(aes(label = round(tot_mean)), color = &#39;black&#39;, vjust = -.5) + # Retirada dos valores do eixo y theme(axis.text.y=element_blank(), axis.ticks.y=element_blank()) + # Aumentar o alcance do eixo y para que os valores não fiquem cortados ylim(0,515) + # Mudar o nome de cada eixo e adicionar um título: xlab(&#39;Tipo Primario&#39;) + ylab(&#39;Media&#39;) + ggtitle(&#39;Media do Total de Atributos por Tipo de Pokemon&#39;) DESAFIO: Cada tipo de pokémon é favorável a um tipo de status. Em relação ao status ofensivo, temos a seguinte pergunta: um tipo é mais favorável ao Ataque ou Ataque Especial? No gráfico abaixo temos uma análise inicial para isto! Execute cada linha do código abaixo separadamente e tente reproduzir e comentar os resultados! pokemon_stats %&gt;% select(Type_1, Attack, Sp_Atk) %&gt;% group_by(Type_1) %&gt;% summarise(atk_minus_sp = mean(Attack) - mean(Sp_Atk)) %&gt;% mutate(pos = (atk_minus_sp &gt; 0)) %&gt;% ggplot(aes(x = Type_1, y = atk_minus_sp, fill = pos)) + geom_col() + scale_fill_manual(values = c(&quot;#ff0841&quot;, &quot;#02031a&quot;), guide = &#39;none&#39;) + labs(y = &#39;Atk - Sp_Atk&#39;, x = &#39;Tipo I&#39;, title = &#39;Qual é a Preferência Ofensiva de um Tipo de Pokémon?&#39;) + geom_text(aes(label = round(atk_minus_sp), y = round(atk_minus_sp) + ifelse(round(atk_minus_sp) &gt;= 0, 3, -3)), position = position_dodge(width = 0.9), colour = &#39;black&#39;) + ylim(-33, 50) 8.4 Gráficos de Linhas (geom_line) Muitas vezes queremos visualizar como uma ou mais variáveis se alteram em relação ao tempo ou algum outra informação. Para isto podemos utilizar gráficos de linhas pela função geom_line. Para isto utilizaremos um conjunto de dados de vendas em razão do tempo: vendas &lt;- read_csv(&quot;G:/Meu Drive/Dados/vendas.csv&quot;) ## Rows: 31 Columns: 2 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): date ## dbl (1): units ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # A data dos dados não está no formato padrão do R str(vendas) ## spec_tbl_df [31 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ date : chr [1:31] &quot;01/01/2010&quot; &quot;02/01/2010&quot; &quot;03/01/2010&quot; &quot;04/01/2010&quot; ... ## $ units: num [1:31] 5064 6115 5305 3185 4182 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. date = col_character(), ## .. units = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # Podemos transformar a data pela função as.Date vendas$date &lt;- as.Date(vendas$date, format = &#39;%d/%m/%y&#39;) # Tabela com todos os formatos possíveis: # https://www.r-bloggers.com/2013/08/date-formats-in-r/ Com os dados tratados podemos então elaborar o gráfico: ggplot(vendas, aes(x = date, y = units)) + geom_line(size = 1) Podemos alterar o formato da linha, adicionar pontos a cada informação observada dentre outras alterações: ggplot(vendas, aes(x = date, y = units)) + geom_line(size = 1, linetype = &#39;dashed&#39;) + geom_point(size = 2) 8.5 Gráfico de Funções (stat_function) Além de visualizar gráficos, podemos utilizar a função geom_line para visualização de funções: ggplot(data.frame(x = c(0, 20)), aes(x = x)) + stat_function(fun = cos, geom = &#39;line&#39;, aes(colour = &#39;cos&#39;), size = 1) + stat_function(fun = sin, geom = &#39;line&#39;, aes(colour = &#39;sin&#39;), size = 1) + theme(legend.position = &#39;top&#39;) + labs(title = &#39;Gráfico de Funções&#39;, colour = &#39;Legenda&#39;) 8.6 Histogramas (geom_histogram) Histogramas são bem úteis para termos uma intuição sobre a distribuição dos dados, principalmente se uma determinada variável possui ou não distribuição normal. Para criarmos basta chamarmos a função geom_histogram: ggplot(data = pokemon_stats, aes(x = Total)) + geom_histogram(binwidth = 50, color = &#39;black&#39;, fill = &#39;blue&#39;, alpha = .3,) + # Facilita a visualização da variável no eixo x: scale_x_continuous(breaks = seq(150, 700, 50)) Há duas formas de criarmos um histograma: fornecendo uma quantidade de bins, os retângulos do gráfico, ou estabelecendo um tamanho a cada retângulo com o parâmetro binwidth. 8.7 Boxplots (geom_boxplot) Boxplots são gráficos que resumem a distribuição do nosso conjunto de dados, além de indicar possíveis outliers: ggplot(pokemon_stats, aes(x = Type_1, y = Total, fill = Type_1)) + geom_boxplot() + theme(legend.position = &#39;none&#39;) 8.8 Facets (facet_grid) Facets são a forma que a biblioteca ggplot2 utiliza para realizar vários tipos de mesmo gráfico de acordo com variações em grupos. Os gráficos são criados normalmente e chamamos a função facet_grid para definir quais tipos de dados serão utilizados para cada linha ou coluna. ggplot(data = pokemon_stats, aes(x = Speed, y = Attack, color = isLegendary)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, col = &quot;black&quot;, formula = y ~ x) + facet_grid(isLegendary~.) + theme(legend.position = &quot;top&quot;) ggplot(data = pokemon_stats, aes(x = Speed, y = Attack, color = isLegendary)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, col = &quot;black&quot;, formula = y ~ x) + facet_grid(.~isLegendary) + theme(legend.position = &quot;top&quot;) 8.9 Bibliotecas Auxiliares Além disso, há diversas bibliotecas que interagem com a ggplot2 para a criação de gráficos. Vamos há algumas e seus exemplos: 8.9.1 cowplot Biblioteca que facilita a criação de vários gráficos em uma mesma figura: library(cowplot) poke1 &lt;- ggplot(data = pokemon_stats, aes(x = Type_1, fill = Type_1)) + geom_bar() + theme(legend.position = &#39;none&#39;) + geom_text(aes(label = ..count..), stat = &quot;count&quot;, color = &#39;black&#39;, vjust = -1) + ylim(0,110) + theme(axis.text.y=element_blank(), axis.ticks.y=element_blank()) + xlab(&#39;Tipo Primario&#39;) + ylab(&#39;Quantidade&#39;) + ggtitle(&#39;Quantidade de Pokemons por Tipo&#39;) poke2 &lt;- pokemon_stats %&gt;% drop_na %&gt;% ggplot(aes(x = Type_2, fill = Type_2)) + geom_bar() + theme(legend.position = &#39;none&#39;) + geom_text(aes(label = ..count..), stat = &quot;count&quot;, color = &#39;black&#39;, vjust = -1) + ylim(0,110) + theme(axis.text.y=element_blank(), axis.ticks.y=element_blank()) + xlab(&#39;Tipo Secundário&#39;) + ylab(&#39;Quantidade&#39;) + ggtitle(&#39;Quantidade de Pokemons por Tipo&#39;) ggdraw(xlim = c(0,2), ylim = c(0,2)) + draw_plot(poke2, x = 0, y = 0, width = 2, height = 1) + draw_plot(poke1, x = 0, y = 1, width = 2, height = 1) 8.9.2 GGally A biblioteca GGally adiciona diversas funções ao ggplot2 principalmente relacionadas a reduzir a complexidade na hora de criar vários gráficos de uma vez: library(GGally) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 pokemon_stats %&gt;% select(Total, Attack, Defense, Sp_Atk, Sp_Def, Speed) %&gt;% ggpairs(diag = list(continuous = wrap(&#39;barDiag&#39;, binwidth = 50, fill = &#39;blue&#39;, alpha = 0.2)), progress = FALSE) "],["introdução-ao-machine-learning.html", "Capítulo 9 Introdução ao Machine Learning", " Capítulo 9 Introdução ao Machine Learning "],["regressão-linear-simples.html", "Capítulo 10 Regressão Linear Simples 10.1 Criando o Modelo &amp; Estimando Seus Coeficientes 10.2 A Poeira Debaixo do Tapete 10.3 Boas Práticas Para Modelagem 10.4 Lidando com Dados Categóricos", " Capítulo 10 Regressão Linear Simples Em problemas de aprendizagem supervisionada regressão linear é um dos algoritmo que inicialmente trabalhamos, pela sua praticidade em extrair informações entre as covariáveis para gerar insights e ser de fácil visualização. Consideremos uma variável resposta \\(Y\\) através de uma variável preditora \\(X\\). No contexto da regressão nós assumimos que exista uma relação linear entre \\(X\\) e \\(Y\\), qual pode ser escrita da seguinte forma \\[ Y \\approx \\beta_0 + \\beta_1 X. \\] Onde \\(\\approx\\) significa que \\(Y\\) é aproximadamente uma soma linear de \\(X\\). Como exemplo, no conjunto mtcars \\(Y\\) pode ser a variável mpg(minhas por galão) enquanto \\(X\\) a variável hp(cavalos de potência). \\[ \\text{mpg} \\approx \\beta_0 + \\beta_1 \\text{hp}. \\] Nestas equações os parâmetros \\(\\beta_0\\) e \\(\\beta_1\\) são duas constantes desconhecidas que representam o intercepto e o coeficiente angular de uma curva. Chamamos estes de coeficientes ou parâmetros do modelo de regressão linear. 10.1 Criando o Modelo &amp; Estimando Seus Coeficientes Na prática os parâmetros do modelo são desconhecidos. Para estíma-los consideremos que temos um conjunto de dados com \\(n\\) linhas: \\[ (x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n) \\] Cada um representando um valor para as colunas \\(X\\) e \\(Y\\). Nosso objetivo é estimar \\(\\beta_0\\) e \\(\\beta_1\\) de modo que o modelo linear realize boas previsões, isto é \\[ y_i \\approx \\beta_0 + \\beta_1x_i, i=1,\\dots, n \\] Desta forma estamos interessados na reta que melhor se aproxime aos pontos do conjunto de dados: ggplot(mtcars, aes(x = hp, y = mpg)) + geom_point() Podemos criar visualmente esta reta utilizando o pacote ggplot2: ggplot(mtcars, aes(x = hp, y = mpg)) + geom_point() + geom_smooth(formula = y ~ x,method = &#39;lm&#39;, se = FALSE) + labs(title = &#39;Regressão linear no conjunto mtcars&#39;) Podemos utilizar a função lm do próprio R para ajustarmos o modelo: # No primeiro parâmetro temos a relação que queremos pro # conjunto de dados # Neste caso estamos querendo prever mpg através de hp # No segundo parâmetro inserimos o conjunto de dados mtcars_lm &lt;- lm(mpg ~ hp, data = mtcars) São várias as vantagens de utilizar a função lm - veremos mais adiante como ajustar o modelo com várias conjuntos do banco de dados. Além disso, a função pode realizar previsões em novos conjuntos, além de carregar diversas informações estatísticas importantes: summary(mtcars_lm) ## ## Call: ## lm(formula = mpg ~ hp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7121 -2.1122 -0.8854 1.5819 8.2360 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.09886 1.63392 18.421 &lt; 2e-16 *** ## hp -0.06823 0.01012 -6.742 1.79e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.863 on 30 degrees of freedom ## Multiple R-squared: 0.6024, Adjusted R-squared: 0.5892 ## F-statistic: 45.46 on 1 and 30 DF, p-value: 1.788e-07 As mais importantes de observamos em um primeiro momento são os coeficientes estimados da nossa curva, neste caso o modelo ajustado é dado por: \\[ \\text{mpg} \\approx 30.09886 - 0.06823 \\text{hp}. \\] Isto significa que ao acréscimo de 1 cavalo de potência em um carro, neste conjunto de dados, temos em média uma diminuição na quantidade de milhas feita por galão, ou seja, o carro acaba gastando mais combustível. Mas afinal, qual é a matemática por trás deste problema? Apesar de termos funções prontas para realizarmos a modelagem, é importante entender a sua criação, pois diversos conceitos partem desta. Para elaborarmos o modelo utilizamos a minimização dos mínimos quadrados. Para isto partimos do modelo comentado inicialmente: \\[ Y \\approx \\beta_0 + \\beta_1 X. \\] Neste modelo, para cada linha do nosso conjunto de dados, \\(y_i\\), teremos uma previsão, \\(\\hat{y_i}\\). Chamamos de i-ésimo resíduo o erro cometido entre o valor real de uma observação e o valor ajustado do modelo em uma observação: \\[ e_i = y_i - \\hat{y_i} \\] data.frame(&#39;hp&#39; = mtcars$hp, &#39;mpg&#39; = mtcars$mpg, &#39;fitted&#39; = mtcars_lm$fitted.values) %&gt;% ggplot(aes(x = hp, y = mpg)) + geom_point() + geom_smooth(formula = y ~ x,method = &#39;lm&#39;, se = FALSE) + geom_segment(aes(x = hp, y = mpg, xend = hp, yend = fitted)) + labs(title = &#39;Regressão linear no conjunto mtcars&#39;) Definimos como soma dos quadrados dos resíduos o valor \\[ \\text{RSS} = e_1^2+e_2^2+\\dots + e_n^2, \\] Estamos interessados na reta que tenha a menor soma dos quadrados dos resíduos. Para avançarmos nesta procura, relembremos que se denotarmos \\(\\hat{y_i}\\) como sendo a previsão do modelo para i-ésima observação, então: \\[ \\hat{y_i} = \\beta_0 + \\beta_1 x_i, i =1,\\dots, n \\] Assim, podemos reescrever os resíduos da seguinte maneira: \\[ \\text{RSS} = (y_1- \\beta_0-\\hat{\\beta_1}x_1)^2 + (y_2- \\beta_0-\\hat{\\beta_1}x_2)^2 +\\cdots + (y_n- \\beta_0-\\hat{\\beta_1}x_n)^2 \\] Assim, para criar o modelo basta considerarmo \\(\\hat{\\beta_0}\\) e \\(\\hat{\\beta_1}\\) que minimizem esta soma. Isto é facilmente resolvido com conceitos de Cálculo 1, onde os melhores parâmetros são: \\[ \\hat{\\beta_1} = \\dfrac{\\sum_{i=1}^n (x_i - \\overline{x})(y_i- \\overline{y})}{\\sum_{i=1}^n (x_i-\\overline{x})^2}, \\quad \\hat{\\beta_0}=\\overline{y} - \\hat{\\beta_1}\\overline{x}. \\] IMPORTANTE: Aqui não precisamos entender diretamente como a fórmula dos parâmetros é encontrada, mas sim o objetivo para chegarmos até essa fórmula. Não é importante lembrar a fórmula para encontrar os parâmetros, mas sim que a reta é encontrada de forma a minimizarmos a soma dos quadrados dos resíduos. Podemos então realizarmos a nossa própria função para obter o nosso modelo de regressão: linear_regression &lt;- function(X, Y){ numerador = sum((X - mean(X)) * (Y - mean(Y))) denominador = sum((X - mean(X))**2) beta_1 = numerador/denominador beta_0 = mean(Y) - beta_1 * mean(X) return(data.frame(&#39;beta_0&#39; = beta_0, &#39;beta_1&#39; = beta_1)) } linear_regression(X = mtcars$hp, Y = mtcars$mpg) ## beta_0 beta_1 ## 1 30.09886 -0.06822828 10.2 A Poeira Debaixo do Tapete A elaboração de um modelo de regressão linear é extremamente simples. Porém, existem diversas condições para a criação deste modelo. Estas condições são teóricas e não irão acarretar em um erro na hora de executar o script no R, porém podem acarretar em erro de interpretação e previsão para dados futuros. As 3 condições que devemos ter para um modelo de regressão linear são: Linearidade nos dados; Erros normalmente distribuídos; Erros com variância igual (Homocedasticidade). 10.2.1 Linearidade nos Dados Linearidade é a hipótese de que de fato há uma relação linear entre os dados para que o modelo esteja bem ajustado, considere o seguinte exemplo: ggplot(regressao_nao_linear, aes(x = X, y = y)) + geom_point() + geom_smooth(method = &#39;lm&#39;, formula = &#39;y ~ x&#39;, se = F) + labs(title = &#39;Pontos que não possuem relação linear&#39;) É evidente que a reta ajustada aqui é péssima. Para resolvermos este problema, neste exemplo em específico, podemos particionar o conjunto de dados em dois: um para valores positivos de \\(X\\) e outro para negativos. Não há uma única maneira para lidar com a não linearidade dos dados, em regressão linear múltipla, onde utilizamos mais de uma coluna para prever a variável resposta podemos realizar operações de forma a lidar com a falta de linearidade dos dados: # Utilização das colunas X e X^2 para a reta de regressão linear: ggplot(regressao_nao_linear, aes(x = X, y = y)) + geom_point() + geom_smooth(method = &#39;lm&#39;, formula = &#39;y ~ x + I(x^2)&#39;, se = F) + labs(title = &#39;Pontos que não possuem relação linear&#39;) IMPORTANTE: Regressão Linear possui este nome pela linearidade em seus coeficientes, no exemplo acima temos o seguinte modelo: \\[ Y = 0.0009678 -0.0032584 X + 0.9931674 X^2 \\] Assim, apesar de utilizarmos os elementos de uma coluna ao quadrado, os parâmetros estimados estão no modelo de forma linear: \\[ Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 \\] veremos mais sobre isso em regressão linear múltipla. 10.2.1.1 Coeficiente de Correlação de Pearson Podemos exprimir a relação linear entre duas colunas em um valor numérico, qual é de mais fácil interpretação quando estamos lidando com várias colunas ao mesmo tempo. Definimos coeficiente de correlação de Pearson entre \\(X\\) e \\(Y\\) o valor \\[ R = \\dfrac{\\sum_{i=1}^n (x_i-\\overline{x})(y_i-\\overline{y})}{\\sqrt{\\sum_{i=1}^n (x_i-\\overline{x})^2 \\sum_{i=1}^n (y_i-\\overline{y})^2}}. \\] Qual possui as seguintes propriedades: Descreve o quão forte é a relação linear entre duas variáveis; O sinal da correlação indica a direção da associação; Sempre varia de -1 a 1; Não é afetado por mudança na escala das variáveis; É sensível a outliers. Existem várias formas de observar a correlação entre colunas do conjunto de dados, uma que me agrada é a função ggpairs do pacote GGally: ggpairs(mtcars[c(&#39;hp&#39;, &#39;mpg&#39;)]) 10.2.2 Normalidade dos Erros Quando criamos o modelo \\[ Y \\approx \\beta_0 + \\beta_1 X. \\] normalmente assumimos que a aproximação pode ser corrigida por uma variável aleatória \\(\\varepsilon\\) que possui distribuição normal, isto é: \\[ Y = \\beta_0 + \\beta_1 X +\\varepsilon. \\] Em outras palavras, o erro de medição possui distribuição normal. Mas afinal, o que é distribuição é uma distribuição? e mais ainda, o que tem de importante dela ser normal? Sendo bem superficiais, tipos de distribuições surgem da forma em que os dados são distribuídos. Em particular, a distribuição normal é importante pois aparece em diversos problemas reais, além de possuir ótimas propriedades matemáticas. Dentre estas estamos interessados que: A média/mediana dos nossos erros seja 0 - a aleatoridade deve acontecer tanto de forma que estejamos superestimando e subestimando a variável \\(Y\\); Homocedasticidade: os erros variem de forma igual - a aleatoridade de cada medição varia dentro de um alcance, imagine errar o peso de pessoas por gramas e depois começar a termos erros na escala de kg no mesmo conjunto de dados. Existem diversas formas para confirmar a normalidade dos erros, assim como a homocedasticidade: Histograma da variável resposta: Maneira inicial, quando assumimos que os resíduos possuem distribuição normal temos, como consequência, a normalidade da variável resposta que estamos tentando prever, qual pode ser observada pelo seu histograma: ggplot(mtcars, aes(x = mpg)) + geom_histogram(bins = 12, colour = &#39;black&#39;) Histograma dos resíduos: maneira mais intuitiva, observamos se o histograma dos resíduos possuí comportamento normal mtcars_residuos &lt;- data.frame(resid = mtcars_lm$residuals) ggplot(mtcars_residuos, aes(x = resid)) + geom_histogram(bins = 10, colour = &#39;black&#39;) Q-Q Plot: Gráfico que compara os quantis de uma amostra com os de uma distribuição de interesse, para que a hipótese se sustente os pontos devem estar próximos da reta identidade: ggplot(mtcars_residuos, aes(sample = resid)) + stat_qq() + geom_qq_line() + xlab(&#39;Teórico&#39;) + ylab(&#39;Amostral&#39;) + ggtitle(&#39;Q-Q Plot&#39;) Testes de Hipótese: é um conceito da inferência estatística, nele confrontramos duas hipóteses e então elaboramos um teste para testá-las. No nosso contexto as hipóteses a serem consideradas são: \\(H_0\\) (Hipótese nula): A distribuição dos dados é normal; \\(H_1\\) (Hipótese alternativa): A distribuição dos dados não é normal. Cada teste possui duas informações: uma estatística, número que representa um valor de referência a uma distribuição e, mais importante, um p-valor, qual representa a probabilidade de obtermos a amostra da distribuição caso consideremos a hipótese nula como verdadeira: library(nortest) # Testes de Normalidade shapiro.test(mtcars_residuos$resid) # Shapiro-Wilk ## ## Shapiro-Wilk normality test ## ## data: mtcars_residuos$resid ## W = 0.92337, p-value = 0.02568 cvm.test(mtcars_residuos$resid) # Cramer-von Mises ## ## Cramer-von Mises normality test ## ## data: mtcars_residuos$resid ## W = 0.11828, p-value = 0.05976 lillie.test(mtcars_residuos$resid) # Lilliefors ## ## Lilliefors (Kolmogorov-Smirnov) normality test ## ## data: mtcars_residuos$resid ## D = 0.11669, p-value = 0.3257 Rejeitamos a hipótese nula caso o p-valor esteja abaixo de \\(0.1,0.05\\) ou \\(0.01\\). IMPORTANTE: a definição formal de p-valor é a probabilidade de que a estatística seja igual ou maior do que a encontrada, e é um pouco mais abstrata. Caso forem estudar sobre testes de hipótese, não se assuste com outras terminologias. Gráfico de covaríaveis x resíduos: Importante para visualizar se os resíduos possuem a mesma variância. Caso isto seja verdadeiro, os pontos estão dentro de um “retângulo imaginário” dentro do eixo y, caso contrário temos gráficos em forma de funil. data.frame(&#39;hp&#39; = mtcars$hp, &#39;resid&#39; = mtcars_residuos$resid) %&gt;% ggplot(aes(x = hp, y = resid)) + geom_point() + geom_abline(slope = 0) + ggtitle(&#39;HP vs Resíduos&#39;) Vejamos um caso onde a variância para ambos os conjuntos é diferente: https://gallery.shinyapps.io/slr_diag/ 10.3 Boas Práticas Para Modelagem Muitas vezes quando ajustamos o modelo acabamos por realizar extrapolações que não possuem base nos dados utilizados. As mais comuns são: Realizar previsões onde não temos dados: Vamos utilizar o exemplo do mtcars. A base mtcars é extraída da revista Motor Trend US de 1974. Imagine que utilizemos este conjunto para realizar previsões de carros de hoje em dia, parece um pouco absurdo, não? Assim, é importante termos cuidado da região e período dos nossos dados, uma vez que diversas alterações acontecem ao decorrer do tempo e nosso modelo estaria desatualizado para análises com carros da atualidade. Outro tipo de extrapolação, também neste mesmo tópico é em realizar previsões fora do alcance do modelo. Vamos imaginar que estejamos interessados em realizar previsões sobre carros com 800, 900 ou até 1000 cavalos de potência. Nosso modelo foi ajustado entre carros com 50 a 350 cavalos de potência - nada sabemos se a relação de linearidade dos dados, normalidade e homocedasticidade dos resíduos irá permanecer para dados neste alcance. 10.4 Lidando com Dados Categóricos Também podemos utilizar dados categóricos para a elaboração de um modelo de regressão. Aqui perdemos um pouco a noção geométrica do modelo, mas ainda sim este pode ser elaborado. Vamos carregar um conjunto de dados criado artificialmente de cargo de cientistas de dados e seus respectivos salários chamado cat_lm: Para trabalharmos com dados categóricos no R precisamos que estes estejam convertidos em fatores: cat_lm$Cargo &lt;- as.factor(cat_lm$Cargo) Agora podemos ajustar o modelo de regressão linear utilizando o cargo de cada pessoa e seu respectivo salário: lm(Salario ~ Cargo, data = cat_lm) ## ## Call: ## lm(formula = Salario ~ Cargo, data = cat_lm) ## ## Coefficients: ## (Intercept) CargoPleno CargoSenior ## 1998 1002 2004 \\(\\color{skyblue}{\\textbf{Questão 1:}}\\) Realize tal coisa "],["regressão-logística.html", "Capítulo 11 Regressão Logística", " Capítulo 11 Regressão Logística library(ggplot2) library(tidyr) library(dplyr) "],["machine-learning-from-disaster.html", "Capítulo 12 Machine Learning from Disaster", " Capítulo 12 Machine Learning from Disaster Todo mundo já assistiu, ou pelo menos ouviu falar, sobre o desastre do navio Titanic. Incrivelmente este caso também pode ser estudado utilizando aprendizado de máquina! Na verdade este é um dos primeiros desafios que trabalhamos quando estudamos nossos primeiros algoritmos. O conjunto de dados e suas informações pode ser encontrado no site Kaggle, um site que hospeda diversos conjuntos de dados e competições de machine learning. Na aula aprenderemos como baixamos e analisamos as observaçõs contidas nesse conjunto de dados, qual pode ser visualizado na tabela abaixo: library(readr) titanic_train &lt;- read_csv(&quot;G:/Meu Drive/Dados/titanic_train.csv&quot;) O objetivo do trabalho é prever se um passageiro sobreviveu ou não ao naufrágio do Titanic. Para começarmos trabalharemos com a função str para observarmos os tipos de variáveis presentes na tabela: str(titanic_train) ## spec_tbl_df [891 × 12] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ PassengerId: num [1:891] 1 2 3 4 5 6 7 8 9 10 ... ## $ Survived : num [1:891] 0 1 1 1 0 0 0 0 1 1 ... ## $ Pclass : num [1:891] 3 1 3 1 3 3 1 3 3 2 ... ## $ Name : chr [1:891] &quot;Braund, Mr. Owen Harris&quot; &quot;Cumings, Mrs. John Bradley (Florence Briggs Thayer)&quot; &quot;Heikkinen, Miss. Laina&quot; &quot;Futrelle, Mrs. Jacques Heath (Lily May Peel)&quot; ... ## $ Sex : chr [1:891] &quot;male&quot; &quot;female&quot; &quot;female&quot; &quot;female&quot; ... ## $ Age : num [1:891] 22 38 26 35 35 NA 54 2 27 14 ... ## $ SibSp : num [1:891] 1 1 0 1 0 0 0 3 0 1 ... ## $ Parch : num [1:891] 0 0 0 0 0 0 0 1 2 0 ... ## $ Ticket : chr [1:891] &quot;A/5 21171&quot; &quot;PC 17599&quot; &quot;STON/O2. 3101282&quot; &quot;113803&quot; ... ## $ Fare : num [1:891] 7.25 71.28 7.92 53.1 8.05 ... ## $ Cabin : chr [1:891] NA &quot;C85&quot; NA &quot;C123&quot; ... ## $ Embarked : chr [1:891] &quot;S&quot; &quot;C&quot; &quot;S&quot; &quot;S&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. PassengerId = col_double(), ## .. Survived = col_double(), ## .. Pclass = col_double(), ## .. Name = col_character(), ## .. Sex = col_character(), ## .. Age = col_double(), ## .. SibSp = col_double(), ## .. Parch = col_double(), ## .. Ticket = col_character(), ## .. Fare = col_double(), ## .. Cabin = col_character(), ## .. Embarked = col_character() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; Temos dados do tipo texto e numéricos. Inicialmente excluíremos do conjunto de dados os nomes, por representarem atributos que não são relevantes para previsões: titanic_train &lt;- titanic_train %&gt;% select(-c(PassengerId, Name, Ticket, Cabin)) Além disto, vamos transformar os dados categóricos em fatores: cols &lt;- c(&#39;Survived&#39;, &#39;Pclass&#39;, &#39;Embarked&#39;) titanic_train[cols] = lapply(titanic_train[cols], factor) Observemos agora se nosso conjunto de dados possui valores faltantes: summary(titanic_train) ## Survived Pclass Sex Age SibSp ## 0:549 1:216 Length:891 Min. : 0.42 Min. :0.000 ## 1:342 2:184 Class :character 1st Qu.:20.12 1st Qu.:0.000 ## 3:491 Mode :character Median :28.00 Median :0.000 ## Mean :29.70 Mean :0.523 ## 3rd Qu.:38.00 3rd Qu.:1.000 ## Max. :80.00 Max. :8.000 ## NA&#39;s :177 ## Parch Fare Embarked ## Min. :0.0000 Min. : 0.00 C :168 ## 1st Qu.:0.0000 1st Qu.: 7.91 Q : 77 ## Median :0.0000 Median : 14.45 S :644 ## Mean :0.3816 Mean : 32.20 NA&#39;s: 2 ## 3rd Qu.:0.0000 3rd Qu.: 31.00 ## Max. :6.0000 Max. :512.33 ## A coluna de idade apresenta 177 valores faltantes, ou seja aproximadamente \\(20\\%\\) dos dados não estão com o atributo idade. Existem diversas formas, com as mais diversas teorias para lidarmos com estes valores, aqui vão algumas: Se a quantidade de valores faltantes é muito grande, retiramos a coluna do conjunto de dados; Se a quantidade é muito pequena, retiramos a linha do conjunto de dados; Completar estes valores com a média da coluna para valores numéricos ou com a moda para valores categóricos. O que é uma quantidade muito grande ou pequena de valores faltantes? Isto depende do problema! Temos 2 valores faltantes na coluna Embarked que, ao serem retirados, provavelmente não causem nenhum impacto no modelo final. Aqui é importante utilizarmos o nosso bom senso, além de testarmos as mais diversas possibilidades, uma vez que alguns modelos podem ser fortemente por outiliers, por exemplo. Vamos então excluir os valores faltantes de Embarked e completar a coluna Age com a média da coluna: titanic_train &lt;- titanic_train %&gt;% drop_na(Embarked) E completar os valores da idade com a média: titanic_train$Age &lt;- titanic_train$Age %&gt;% replace_na(mean(titanic_train$Age, na.rm = T)) Dessa realizamos um preparo inicial para o conjunto de dados! A criação do algoritmo de regressão logística é simples: titanic_reg &lt;- glm(Survived ~ ., data = titanic_train, family = binomial(link = &#39;logit&#39;)) Para carregarmos o conjunto de teste e então realizarmos previsões: titanic_test &lt;- read_csv(&quot;G:/Meu Drive/Dados/titanic_test.csv&quot;) Lembrando que as colunas do conjunto de teste, qual queremos realizar as previsões deve estar de acordo com o de treinamento! titanic_test &lt;- titanic_test %&gt;% select(-c(PassengerId, Name, Ticket, Cabin)) cols &lt;- c(&#39;Pclass&#39;, &#39;Embarked&#39;) titanic_test[cols] = lapply(titanic_test[cols], factor) titanic_test$Age &lt;- titanic_test$Age %&gt;% replace_na(mean(titanic_test$Age, na.rm = T)) # A coluna Fare apresenta um valor NA e este será atribuído a média: titanic_test$Fare &lt;- titanic_test$Fare %&gt;% replace_na(mean(titanic_test$Fare, na.rm = T)) Para porfim realizarmos as previsões: probabilidades &lt;- predict(titanic_reg, titanic_test, type = &quot;response&quot;) classes_prev &lt;- ifelse(probabilidades &gt; 0.5, 1, 0) Por fim, criemos o conjunto de dados para ser enviado ao Kaggle: kaggle_titanic &lt;- data.frame(PassengerId = 892:1309, Survived = classes_prev) Por fim, salvemos o nosso data.frame em um arquivo csv: write.csv(kaggle_titanic,&quot;kaggle_test.csv&quot;, row.names = FALSE) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
